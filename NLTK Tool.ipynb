{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527def0f-aaf6-41c7-b790-5525c8a8fb4d",
   "metadata": {},
   "source": [
    "## A tool that can extract text from websites and perform analysis such as:\n",
    "### - Sentiment Analysis: \n",
    "Determine the sentiment or opinion expressed in the            text, whether it is positive, negative, or neutral.\n",
    "\n",
    "### - Entity Recognition: \n",
    "Identify and extract named entities from the text, such as people, organizations, locations, or specific terms related to your domain. \n",
    "\n",
    "### - Keyword Extraction: \n",
    "Extract important keywords or key phrases from the text to understand the main topics or themes discussed\n",
    "\n",
    "### - Visualization and Reporting: \n",
    "Present the results of the analysis through visualizations and reports. You can use libraries like Matplotlib, Seaborn, or Plotly to create charts, graphs, or word clouds to visualize the extracted insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f113fb3-f7bb-481a-867e-94bd1c637358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pyphen\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ed7dc-9e57-4ff7-8140-f705c602061b",
   "metadata": {},
   "source": [
    "Import the dataset or the URL to loop through.\n",
    "Import dictionaries such as Stopwords, Generic Words and etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd818923-4907-4ff4-aa19-4a859134fc25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for url in df['URL']:\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract the article title\n",
    "    try:\n",
    "        title = soup.find('h1').text.strip()\n",
    "\n",
    "        # Extract the article text\n",
    "        text = f'{title}\\n'\n",
    "        for p in soup.find_all('p', class_=lambda x: x != 'tdm-descr'):\n",
    "            if not p.find_parents('ul'):\n",
    "                text += p.text.strip() + ' '\n",
    "        n = n+1\n",
    "        # Print the extracted information\n",
    "        print(\"Title:\", title)\n",
    "    \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    def filtered_sentence_as_lst(text):\n",
    "        word_tokens = word_tokenize(text)\n",
    "        # converts the words in word_tokens to lower case and then checks whether\n",
    "        #they are present in stop_words or not\n",
    "        # filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "        #with no lower case conversion\n",
    "        filtered_sentence_as_lst = []\n",
    "        c= 0\n",
    "        for w in word_tokens:\n",
    "                if w.lower() not in stop_words:\n",
    "                    filtered_sentence_as_lst.append(w)\n",
    "                else:\n",
    "                    c += 1\n",
    "  \n",
    "        return filtered_sentence_as_lst\n",
    "\n",
    "    def filtered_sentence(a):\n",
    "        filtered_sentence = (' ').join(a)\n",
    "        return filtered_sentence\n",
    "\n",
    "\n",
    "    def remove_special_characters_from_filtered(text):\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'  # Matches any character that is not alphanumeric or whitespace\n",
    "        # a = []\n",
    "        # for i in text:\n",
    "        cleaned_text = re.sub(pattern, '', text)\n",
    "            # if cleaned_text != '':\n",
    "            # a.append(cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    remove_special_characters_from_filtered = remove_special_characters_from_filtered(filtered_sentence(filtered_sentence_as_lst(text)))\n",
    "\n",
    "    def remove_special_characters_from_text(text):\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'   \n",
    "        cleaned_text = re.sub(pattern, '', text)\n",
    "        return cleaned_text\n",
    "    remove_special_characters_from_text = remove_special_characters_from_text(text)\n",
    "\n",
    "    positive_words = set(positive_words)\n",
    "    negative_words = set(negative_words)\n",
    "\n",
    "    def positive_score(text):\n",
    "        p = 0   \n",
    "        for w in filtered_sentence_as_lst(text):\n",
    "            if w.lower() in positive_words:\n",
    "                p += 1\n",
    "        return p\n",
    "    positive_score = positive_score(text)\n",
    "\n",
    "    def negative_score(text):\n",
    "        n = 0   \n",
    "        for w in filtered_sentence_as_lst(text):\n",
    "            if w.lower() in negative_words:\n",
    "                n += 1\n",
    "        return n\n",
    "    negative_score = negative_score(text)\n",
    "\n",
    "    def polarity_score(p, n):\n",
    "        polarity_score = (p - n)/((p + n) + 0.000001)\n",
    "        return polarity_score\n",
    "\n",
    "    polarity_score = polarity_score(positive_score, negative_score)\n",
    "\n",
    "    def subjectivity_score(p, n, a):\n",
    "        subjectivity_score = (p + n) / (len(a.split()) + 0.000001)\n",
    "        return subjectivity_score\n",
    "\n",
    "    subjectivity_score = subjectivity_score(positive_score, negative_score, remove_special_characters_from_filtered)\n",
    "\n",
    "    def sentence_count(text):\n",
    "        return text.count('.') + text.count('?') + text.count('!')\n",
    "\n",
    "    def word_count(text):\n",
    "        return len(text.split())\n",
    "\n",
    "    def ave_sentence_length(a,b):\n",
    "        ave_sentence_length = a / b\n",
    "        return ave_sentence_length\n",
    "\n",
    "    ave_sentence_length = ave_sentence_length(word_count(text), sentence_count(text))\n",
    "\n",
    "    def syllable_per_normal_word(text):\n",
    "        text_list = text.split()\n",
    "        def count_syllables(word):\n",
    "            dic = pyphen.Pyphen(lang='en')\n",
    "            syllables = dic.inserted(word).count('-') + 1\n",
    "            return syllables\n",
    "\n",
    "        acronym_count = []\n",
    "        for acr in text_list:\n",
    "            if acr.upper() == acr:\n",
    "                acronym_count.append(acr)\n",
    "                text_list.remove(acr)\n",
    "        syllable_per_normal_word = {}\n",
    "\n",
    "        for word in text_list:\n",
    "            syllable_count = count_syllables(word)\n",
    "            if syllable_count <= 2:\n",
    "                syllable_per_normal_word[word] = syllable_count\n",
    "        return syllable_per_normal_word\n",
    "    syllable_per_normal_word = syllable_per_normal_word(text)\n",
    "\n",
    "    \n",
    "    def syllable_per_complex_word(text):\n",
    "        text_list = text.split()\n",
    "        def count_syllables(word):\n",
    "            dic = pyphen.Pyphen(lang='en')\n",
    "            syllables = dic.inserted(word).count('-') + 1\n",
    "            return syllables\n",
    "\n",
    "        acronym_count = []\n",
    "        for acr in text_list:\n",
    "            if acr.upper() == acr:\n",
    "                acronym_count.append(acr)\n",
    "                text_list.remove(acr)\n",
    "        syllable_per_complex_word = {}\n",
    "\n",
    "        for word in text_list:\n",
    "            syllable_count = count_syllables(word)\n",
    "            if syllable_count > 2:\n",
    "                syllable_per_complex_word[word] = syllable_count\n",
    "        return syllable_per_complex_word\n",
    "    syllable_per_complex_word = syllable_per_complex_word(text)\n",
    "    \n",
    "    \n",
    "    def syllable_per_word(a,b):\n",
    "        syllable_per_word = (sum(a.values()) + sum(b.values())) / \\\n",
    "                            (len(a) + len(b))\n",
    "        return syllable_per_word\n",
    "    syllable_per_word = syllable_per_word(syllable_per_complex_word, syllable_per_normal_word)\n",
    "\n",
    "   \n",
    "    def number_of_complex_words(a):\n",
    "        number_of_complex_words = len(a)\n",
    "        return number_of_complex_words\n",
    "\n",
    "    number_of_complex_words = number_of_complex_words(syllable_per_complex_word)\n",
    "\n",
    "    def number_of_words_in_text(a):\n",
    "        number_of_words_in_text = len(a.split())\n",
    "        return number_of_words_in_text\n",
    "\n",
    "    number_of_words_in_text = number_of_words_in_text(remove_special_characters_from_text)\n",
    "\n",
    "    def complex_words_percentage(a,b):\n",
    "        complex_words_percentage = (a / b) * 100\n",
    "        return complex_words_percentage\n",
    "\n",
    "    complex_words_percentage = complex_words_percentage(number_of_complex_words, number_of_words_in_text)\n",
    "\n",
    "\n",
    "    def fog_index(a,b):\n",
    "        fog_index = 0.4 * (a + b)\n",
    "        return fog_index\n",
    "\n",
    "    fog_index = fog_index(ave_sentence_length, complex_words_percentage)\n",
    "\n",
    "\n",
    "    def pronouns1(text):\n",
    "        pattern = r\"\\b(we|I|my|us|ours)\\b\"\n",
    "        pronouns1 = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "        return len(pronouns1)\n",
    "    \n",
    "    \n",
    "    def clean_words_list(a):\n",
    "        clean_words_list = a.split()\n",
    "        return clean_words_list\n",
    "    clean_words_list = clean_words_list(remove_special_characters_from_filtered)\n",
    "\n",
    "    def words_list(a):\n",
    "        words_list = a.split()\n",
    "        return words_list\n",
    "    words_list = words_list(text)\n",
    "\n",
    "    def total_characters(a):\n",
    "        total_characters = sum(len(word) for word in a)\n",
    "        return total_characters\n",
    "    total_characters = total_characters(clean_words_list)\n",
    "\n",
    "    def total_words(a):\n",
    "        total_words = len(a)\n",
    "        return total_words\n",
    "    total_words = total_words(words_list)\n",
    "\n",
    "\n",
    "    def average_word_length(a, b):\n",
    "        average_word_length = a / b\n",
    "        return average_word_length\n",
    "    average_word_length = average_word_length(total_characters, total_words)\n",
    "\n",
    "    def total_cleaned_words(a):\n",
    "        total_cleaned_words = len(a)\n",
    "        return total_cleaned_words\n",
    "\n",
    "    total_cleaned_words = total_cleaned_words(clean_words_list)\n",
    "\n",
    "    # total_cleaned_words\n",
    "\n",
    "    def ave_number_of_words_per_sentence(a, b):\n",
    "        ave_number_of_words_per_sentence = a / b\n",
    "        return ave_number_of_words_per_sentence\n",
    "\n",
    "    ave_number_of_words_per_sentence = ave_number_of_words_per_sentence(total_words, sentence_count(filtered_sentence_as_lst(text)))\n",
    "    data = {\n",
    "        'URL_ID': n,\n",
    "        'URL': url,\n",
    "        'POSITIVE SCORE': [positive_score],\n",
    "        'NEGATIVE SCORE': negative_score,\n",
    "        'POLARITY SCORE': polarity_score,\n",
    "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
    "        'AVG SENTENCE LENGTH': ave_sentence_length,\n",
    "        'PERCENTAGE OF COMPLEX WORDS': complex_words_percentage,\n",
    "        'FOG INDEX': fog_index,\n",
    "        'AVG NUMBER OF WORDS PER SENTENCE': ave_number_of_words_per_sentence,\n",
    "        'COMPLEX WORD COUNT': number_of_complex_words,\n",
    "        'WORD COUNT': total_words,\n",
    "        'SYLLABLE PER WORD': syllable_per_word,\n",
    "        'PERSONAL PRONOUNS': pronouns1(text),\n",
    "        'AVG WORD LENGTH': average_word_length,\n",
    "    }\n",
    "\n",
    "    new_row_as_df = pd.DataFrame(data)\n",
    "    output_df = pd.concat([output_df, new_row_as_df], ignore_index=True)\n",
    "\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7315c1-11d9-4379-8909-3b206fab2ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_excel('output1.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
